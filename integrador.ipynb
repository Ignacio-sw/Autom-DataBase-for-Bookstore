{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe objective of this script is to create a database of the most sold\\nbooks weekly by scraping the bookstore page, all the data will be stored in \\na Pandas dataframe and later into a mySQL database using mysql.connector\\n\\n'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import pymysql as sql\n",
    "from datetime import datetime\n",
    "\"\"\"\n",
    "The objective of this script is to create a database of the most sold\n",
    "books weekly by scraping the bookstore page, all the data will be stored in \n",
    "a Pandas dataframe and later into a mySQL database using mysql.connector\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting the data from the page\n",
    "#we are given to work with\n",
    "\n",
    "url_books = \"https://cuspide.com/100-mas-vendidos/\"    \n",
    "req_books = requests.get(url_books,'lxml')          \n",
    "\n",
    "\n",
    "#Requesting the data we are later going to use \n",
    "#to convert ARS$ to black market dolar prices in Argentina ('Dolar blue')                                                  \n",
    "\n",
    "url_dolarblue = \"https://www.cronista.com/MercadosOnline/dolar.html\"     \n",
    "req_dolarblue = requests.get(url_dolarblue, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dolarblue = \"https://www.cronista.com/MercadosOnline/dolar.html\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using scraping to get book tittles:\n",
    "\n",
    "soup_books = bs.BeautifulSoup(req_books.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_names_h3 = soup_books.find_all('h3',class_=\"name product-title woocommerce-loop-product__title\")\n",
    "\n",
    "#Saving all the titles on list_names for later creation of a df\n",
    "\n",
    "list_names = [i.text for i in list_names_h3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_urls_div=soup_books.find_all(\"div\", class_=\"box-image-top\")\n",
    "\n",
    "#Saving the urls of individual books\n",
    "\n",
    "list_urls=[i.a['href'] for i in list_urls_div]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pages scanned...\n",
      "2 pages scanned...\n",
      "3 pages scanned...\n",
      "4 pages scanned...\n",
      "5 pages scanned...\n",
      "6 pages scanned...\n",
      "7 pages scanned...\n",
      "8 pages scanned...\n",
      "9 pages scanned...\n",
      "10 pages scanned...\n",
      "11 pages scanned...\n",
      "12 pages scanned...\n",
      "13 pages scanned...\n",
      "14 pages scanned...\n",
      "15 pages scanned...\n",
      "16 pages scanned...\n",
      "17 pages scanned...\n",
      "18 pages scanned...\n",
      "19 pages scanned...\n",
      "20 pages scanned...\n",
      "21 pages scanned...\n",
      "22 pages scanned...\n",
      "23 pages scanned...\n",
      "24 pages scanned...\n",
      "25 pages scanned...\n",
      "26 pages scanned...\n",
      "27 pages scanned...\n",
      "28 pages scanned...\n",
      "29 pages scanned...\n",
      "30 pages scanned...\n",
      "31 pages scanned...\n",
      "32 pages scanned...\n",
      "33 pages scanned...\n",
      "34 pages scanned...\n",
      "35 pages scanned...\n",
      "36 pages scanned...\n",
      "37 pages scanned...\n",
      "38 pages scanned...\n",
      "39 pages scanned...\n",
      "40 pages scanned...\n",
      "41 pages scanned...\n",
      "42 pages scanned...\n",
      "43 pages scanned...\n",
      "44 pages scanned...\n",
      "45 pages scanned...\n",
      "46 pages scanned...\n",
      "47 pages scanned...\n",
      "48 pages scanned...\n",
      "49 pages scanned...\n",
      "50 pages scanned...\n",
      "51 pages scanned...\n",
      "52 pages scanned...\n",
      "53 pages scanned...\n",
      "54 pages scanned...\n",
      "55 pages scanned...\n",
      "56 pages scanned...\n",
      "57 pages scanned...\n",
      "58 pages scanned...\n",
      "59 pages scanned...\n",
      "60 pages scanned...\n",
      "61 pages scanned...\n",
      "62 pages scanned...\n",
      "63 pages scanned...\n",
      "64 pages scanned...\n",
      "65 pages scanned...\n",
      "66 pages scanned...\n",
      "67 pages scanned...\n",
      "68 pages scanned...\n",
      "69 pages scanned...\n",
      "70 pages scanned...\n",
      "71 pages scanned...\n",
      "72 pages scanned...\n",
      "73 pages scanned...\n",
      "74 pages scanned...\n",
      "75 pages scanned...\n",
      "76 pages scanned...\n",
      "77 pages scanned...\n",
      "78 pages scanned...\n",
      "79 pages scanned...\n",
      "80 pages scanned...\n",
      "81 pages scanned...\n",
      "82 pages scanned...\n",
      "83 pages scanned...\n",
      "84 pages scanned...\n",
      "85 pages scanned...\n",
      "86 pages scanned...\n",
      "87 pages scanned...\n",
      "88 pages scanned...\n",
      "89 pages scanned...\n",
      "90 pages scanned...\n",
      "91 pages scanned...\n",
      "92 pages scanned...\n"
     ]
    }
   ],
   "source": [
    "request_errors=[]\n",
    "price_ar=[]\n",
    "price_us=[]\n",
    "page=0\n",
    "\n",
    "#Here we scrap the price in both ARS and USD.\n",
    "#Unfortunatly the official price in USD is not accesible from the general url, so its\n",
    "#needed to acces every book url to get this, and slowing the proces big time.\n",
    "#To do this we use the urls already scraped in list_urls\n",
    "for book in list_urls:\n",
    "    try:\n",
    "        req_temp = requests.get(book,'lxml')\n",
    "        instance= bs.BeautifulSoup(req_temp.text)\n",
    "        info=instance.find(\"div\",class_=\"product-info summary col-fit col entry-summary product-summary text-left form-minimal\")\n",
    "        \n",
    "        #When going to append the values are formated so they can be used as int if necessary\n",
    "        price_ar.append(info.find(\"bdi\").text[2:].replace(\".\",\"\").replace(\",\",\".\"))\n",
    "        price_us.append(info.find(\"span\",attrs={\"style\":\"font-size: 1.3em\"}).text.replace(\".\",\"\").replace(\",\",\".\"))\n",
    "\n",
    "        page+=1\n",
    "        print(f\"{page} pages scanned...\")\n",
    "    \n",
    "    #In case of failed request the information of the error is stored in a list of lists\n",
    "    #where every list contains [error, url_where_error], to be used in a table to audit errors\n",
    "    except:\n",
    "        requests.exceptions.HTTPError\n",
    "        print(f\"Could not obtain the data from '{list_names[page]}'\")\n",
    "        request_errors.append([f\"Failed capture of book '{list_names[page]}'\", book])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Having the ARS price we can convert to 'Dolar blue' now\n",
    "\n",
    "price_blue = []\n",
    "try:\n",
    "    inst_blue = bs.BeautifulSoup(req_dolarblue.text)\n",
    "    aux = inst_blue.find_all(\"a\", href=\"/MercadosOnline/moneda.html?id=ARSB\")\n",
    "\n",
    "    for i in aux:\n",
    "        \n",
    "        #I find this to be the shortest path to get the value of blue, some formating is \n",
    "        #used to clean the text,\n",
    "        if ('blue' and  '$') in i.text.lower():\n",
    "            i=i.text\n",
    "            blue=int(i[(i.find('$')+1):i.find(',')])\n",
    "            break\n",
    "        \n",
    "    #Divide ARS by BLUE to convert prices       \n",
    "    for pr in price_ar:\n",
    "\n",
    "        price_blue.append(round(float(pr)/blue,2))\n",
    "\n",
    "\n",
    "#Storing eventual errors as before\n",
    "except:\n",
    "    requests.exceptions.HTTPError\n",
    "    print(f\"Could not obtain the data from {req_dolarblue}\")\n",
    "    request_errors.append([f\"Failed capture of data\", req_dolarblue])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can create the dataframe using a dicctionary based on the lists we been creating:\n",
    "\n",
    "df_books = pd.DataFrame({\"Name\": list_names,  \n",
    "            \"Price ARS\": price_ar, \n",
    "            \"Price USD\": price_us,\n",
    "            \"Price Dolar Blue\": price_blue,\n",
    "            \"URL\": list_urls})\n",
    "\n",
    "today_ts = pd.Timestamp.today()\n",
    "df_books['Update_date'] = today_ts\n",
    "df_books['Update_date'] = pd.to_datetime(df_books['Update_date'].dt.date)\n",
    "\n",
    "#The errors df\n",
    "df_errors = pd.DataFrame({\"Request error\": [i[0] for i in request_errors],\n",
    "                          \"URL\":[i[1] for i in request_errors]})\n",
    "\n",
    "df_errors['Update_date'] = today_ts\n",
    "df_errors['Update_date'] = pd.to_datetime(df_errors['Update_date'].dt.date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to load the data into the database I export them in a .csv file \n",
    "#to later load it in, there is most likely a more eficient way memory wise \n",
    "#but this one solves the problem quite elegantly  \n",
    "\n",
    "csv_to_load_books = df_books.to_csv(\"C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/books_data.csv\",sep=\";\")\n",
    "\n",
    "csv_to_load_errors = df_errors.to_csv(\"C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/errors_data.csv\",sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now having extracted all the data we neew we'll proced to create \n",
    "   the database to store it \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First I use pymysql (as sql) to connect to mysql\n",
    "# and continue programing using mysql sintaxis inside execute() method\n",
    "cnx = sql.connect(\n",
    "    user= 'root',           #Here we set the data we need to connect \n",
    "    passwd = '1234',        # in this example a local server\n",
    "    host = '127.0.0.1',\n",
    "    local_infile=True)\n",
    "\n",
    "\n",
    "cur = cnx.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating database and setting for use\n",
    "cur.execute(        \n",
    "    \"\"\"CREATE DATABASE  IF NOT EXISTS top_books;\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "cur.execute(\n",
    "\n",
    "    \"\"\"USE top_books;\n",
    "    \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the table to store the books\n",
    "cur.execute(\"\"\"\n",
    "\n",
    "    DROP TABLE IF EXISTS books;\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS books(\n",
    "\n",
    "    Idbook INT NOT NULL,\n",
    "    Title VARCHAR(70),\n",
    "    Price_ar FLOAT,\n",
    "    Price_us FLOAT,\n",
    "    Price_blue FLOAT,\n",
    "    Url VARCHAR(100),\n",
    "    Update_date DATE\n",
    "    );\n",
    "    \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data, we store in a .csv file created from the data frame\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "\n",
    "    SET GLOBAL local_infile = 1;\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "\n",
    "    LOAD DATA LOCAL INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/books_data.csv'\n",
    "    INTO TABLE books\n",
    "    CHARACTER SET latin1\n",
    "    FIELDS TERMINATED BY ';'\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    IGNORE 1 LINES\n",
    "    (Idbook,Title, Price_ar, Price_us, Price_blue, Url, Update_date);\n",
    "\n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding a primary key\n",
    "cur.execute( \"\"\"\n",
    "    \n",
    "    ALTER TABLE books\n",
    "    ADD PRIMARY KEY (Idbook);\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the table to store the books\n",
    "cur.execute(\"\"\"\n",
    "\n",
    "    DROP TABLE IF EXISTS errors;\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS errors(\n",
    "\n",
    "    Iderror INT NOT NULL,\n",
    "    Description VARCHAR(100),\n",
    "    Problematic_url Varchar(100),\n",
    "    Update_date DATE\n",
    "    );\n",
    "    \n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the errors data\n",
    "cur.execute(\"\"\"\n",
    "\n",
    "    LOAD DATA LOCAL INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/errors_data.csv'\n",
    "    INTO TABLE errors\n",
    "    CHARACTER SET latin1\n",
    "    FIELDS TERMINATED BY ';'\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    IGNORE 1 LINES\n",
    "    (Iderror, Description, Problematic_url, Update_date);\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute( \"\"\"\n",
    "\n",
    "    ALTER TABLE errors\n",
    "    ADD PRIMARY KEY (Iderror);\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx.close()\n",
    "#Finally we close the connection and the database is done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
